{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "name": "Aphrodite.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlpinDale/misc-scripts/blob/main/Aphrodite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Aphrodite Engine Colab!\n",
        "\n",
        "If you're on mobile, please tab on the play button below. If you're not, you can safely skip it and go to the next cell.\n",
        "\n",
        "The default model is Mythalion-13B, but you can either type in your model name, or select one of the defaults in the dropdown.\n",
        "\n",
        "If you're running a non-quantized model, please set the Quantization to `None` instead. AWQ and GPTQ quantization is also available, but you will need Colab Pro with better GPUs.\n"
      ],
      "metadata": {
        "id": "bN4MWt2g6E9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkXkyiFP2Hq"
      },
      "source": [
        "#@title <b>v-- Tap this if you play on Mobile</b> { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (You can ignore this step if you used run all and are on PC)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y"
      },
      "outputs": [],
      "source": [
        "#@title <b>v-- The free plan on Google Colab only supports up to 13B (in GPTQ/AWQ) formats.</b>\n",
        "#@markdown The free plan on Google Colab only supports up to 13B (in GPTQ/AWQ) formats.\n",
        "#@markdown You can enter a custom model as well (for models bigger than 13B GPTQ/AWQ, you will need Colab Pro).\n",
        "\n",
        "Model = \"TheBloke/Mythalion-13B-AWQ\" #@param [\"TheBloke/Mythalion-13B-AWQ\", \"TheBloke/MythoMax-L2-13B-AWQ\", \"TheBloke/Pygmalion-2-13B-AWQ\", \"TheBloke/MLewd-L2-Chat-13B-AWQ\"]{allow-input: true}\n",
        "Quantization = \"awq\" #@param ['awq', 'gptq', \"None\"]\n",
        "!pip uninstall aphrodite-engine -y\n",
        "!pip install aphrodite-engine\n",
        "!wget -c https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://127.0.0.1:2242 &\n",
        "!sleep 10\n",
        "!cat nohup.out\n",
        "!python -m aphrodite.endpoints.kobold.api_server --model $Model --dtype float16 --host 127.0.0.1 -q $Quantization --gpu-memory-utilization 0.88 &\n"
      ]
    }
  ]
}
