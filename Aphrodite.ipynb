{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "name": "Aphrodite.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlpinDale/misc-scripts/blob/main/Aphrodite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Aphrodite Engine Demo!\n",
        "\n",
        "You can play around with the API here, or scroll down to see how you can interact with the engine using Python.\n",
        "\n",
        "By default, a few models have been included. You can models for a variety of quantization methods, including: EXL2, GPTQ, AWQ, GGUF, Marlin, AQLM, SqueezeLLM, etc. Simply input their Hugging Face ID in the `Model` field, and configure the other options as necessary. Colab only has 16GB of VRAM, so you may be unable to run larger models reliably - a good rule of thumb is taking the model's size in GBs and adding 3 to it. If that's less than ~15, then you can run it here. If you're confused on what a model ID is, it's essentially the username/model-name on Hugging Face. For example, the URL `https://huggingface.co/Kooten/Kunoichi-DPO-v2-7B-8bpw-exl2`'s ID is simply the `https://huggingface.co/` part stripped out.\n",
        "\n",
        "If you're on mobile, please tap on the play button below. If you're not, you can safely skip it and go to the next cell.\n",
        "\n",
        "If you run into any problems, open an issue [here](https://github.com/AlpinDale/misc-scripts/issues).\n",
        "\n",
        "For **20B models**, make sure the model is quantized and set `GPU_Memory_Utilization` to 0.9. Make sure you don't request more than 200 tokens per reply, as it may run out of memory."
      ],
      "metadata": {
        "id": "bN4MWt2g6E9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkXkyiFP2Hq"
      },
      "source": [
        "#@title <b>v-- Tap this if you play on Mobile</b> { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (You can ignore this step if you used run all and are on PC)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <b>v-- Run this cell to start the engine.</b>\n",
        "#@markdown **The free plan on Google Colab only supports up to 13B (quantized).**\n",
        "#@markdown **You can enter a custom model as well in addition to the default ones. Supported models types are:**\n",
        "#@markdown ****\n",
        "\n",
        "Model = \"Kooten/Kunoichi-DPO-v2-7B-8bpw-exl2\" #@param [\"Kooten/Kunoichi-DPO-v2-7B-8bpw-exl2\", \"TheBloke/UNA-TheBeagle-7B-v1-GPTQ\", \"LoneStriker/Fimbulvetr-11B-v2-GPTQ\", \"TheBloke/OpenHermes-2.5-Mistral-7B-AWQ\"]{allow-input: true}\n",
        "#@markdown **The specific model branch to download. Useful for exl2 models where every bpw is on separate branches.**\n",
        "Revision = \"main\" #@param []{allow-input: true}\n",
        "#@markdown **Should be auto-recognized for most models. If you receive a KeyError, or unexpectedly run out of memory for small models, please use this to specify the correct quant format. Most exl2 models have this issue, so configure this for exl2 models.**\n",
        "Quantization = \"exl2\" #@param [\"None\", \"exl2\", \"gptq\", \"awq\", \"aqlm\", \"quip\", \"marlin\"]\n",
        "#@markdown **Adjust this and the Context Length slider if you're running into COOM (CUDA Out Of Memory) issues!**\n",
        "GPU_Memory_Utilization = 0.95 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown **The free Colab GPU may not have enough memory to accomodate more than 8192 Context Length for most models.**\n",
        "Context_Length = 8192 #@param {type:\"slider\", min:1024, max:32768, step:1024}\n",
        "#@markdown **Use FP8 KV Cache to reduce memory usage and allow higher context lengths.**\n",
        "FP8_KV_Cache = False #@param {type:\"boolean\"}\n",
        "#@markdown **Check this to launch a Kobold-compatible API in addition to the OpenAI one. Keep in mind that the API key does not protect Kobold routes.**\n",
        "launch_kobold_api = False #@param {type:\"boolean\"}\n",
        "#@markdown **[OPTIONAL] Enter an API key to secure your API.**\n",
        "OpenAI_API_Key = \"\" #@param []{allow-input: true}\n",
        "\n",
        "\n",
        "%pip show aphrodite-engine &> /dev/null && echo \"Existing Aphrodite Engine installation found. Updating...\" && pip uninstall aphrodite-engine -q -y\n",
        "!echo \"Installing/Updating the Aphrodite Engine, this may take a while...\"\n",
        "%pip install aphrodite-engine --extra-index-url https://downloads.pygmalion.chat/whl > /dev/null 2>&1\n",
        "!echo \"Installation successful! Starting the engine now.\"\n",
        "\n",
        "\n",
        "!wget -q -c https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!echo \"Creating a Cloudflare URL...\"\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://127.0.0.1:2242 &> nohup.out &\n",
        "!sleep 10\n",
        "!echo \"============================================================\"\n",
        "!echo \"Please copy this URL:\"\n",
        "!grep -o 'https://[^ ]*.trycloudflare.com' nohup.out\n",
        "!echo \"============================================================\"\n",
        "\n",
        "model = Model\n",
        "gpu_memory_utilization = GPU_Memory_Utilization\n",
        "context_length = Context_Length\n",
        "api_key = OpenAI_API_Key\n",
        "quant = Quantization\n",
        "fp8_kv = FP8_KV_Cache\n",
        "kobold = launch_kobold_api\n",
        "revision = Revision\n",
        "\n",
        "command = [\n",
        "    \"aphrodite run\",\n",
        "    model,\n",
        "    \"--dtype\", \"float16\",\n",
        "    \"--host\", \"127.0.0.1\",\n",
        "    \"--gpu-memory-utilization\", str(gpu_memory_utilization),\n",
        "    \"--max-model-len\", str(context_length),\n",
        "    \"--max-log-len\", \"0\",\n",
        "    \"--revision\", revision\n",
        "]\n",
        "\n",
        "if kobold:\n",
        "    command.append(\"--launch-kobold-api\")\n",
        "\n",
        "if quant != \"None\":\n",
        "    command.extend([\"-q\", quant])\n",
        "\n",
        "if fp8_kv:\n",
        "    command.append(\"--kv-cache-dtype fp8\")\n",
        "\n",
        "if api_key != \"\":\n",
        "    command.extend([\"--api-keys\", api_key])\n",
        "\n",
        "!{\" \".join(command)} &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Developers\n",
        "\n",
        "You can use the `aphrodite` library to load an LLM and generate text with it (!). To get started, install the package using pip."
      ],
      "metadata": {
        "id": "qPUBgp-tul-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install aphrodite-engine --extra-index-url https://downloads.pygmalion.chat/whl"
      ],
      "metadata": {
        "id": "kJ8cMVofu55A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define some prompts. We'll pass these to the model, and have it generate completions for it."
      ],
      "metadata": {
        "id": "BNJCzjedu9vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Describe a serene and peaceful forest clearing on a warm summer day. Include details about the sights, sounds, and smells that one might experience in this tranquil setting.\",\n",
        "    \"Write a short story about a curious robot named Zephyr who discovers an ancient, mysterious artifact hidden deep within an abandoned factory. The artifact holds a secret that could change the course of robot history.\",\n",
        "    \"Create a dialogue between two friends discussing their dreams and aspirations for the future. One friend is an optimist, while the other is more pragmatic and cautious.\",\n",
        "    \"Compose a haiku about the beauty and simplicity of a single cherry blossom falling from a tree in the springtime.\",\n",
        "]"
      ],
      "metadata": {
        "id": "q1epOK8-vJ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll import the `LLM` and `SamplingParams` classes from Aphrodite. The `LLM` class handles the model loading, and all the configuration related to it, and `SamplingParams` handles the sampler settings."
      ],
      "metadata": {
        "id": "2fLP8TVzwAm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aphrodite import LLM, SamplingParams\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.9, min_p=0.1, max_tokens=256)\n",
        "\n",
        "llm = LLM(model=\"Kooten/Kunoichi-DPO-v2-7B-8bpw-exl2\", quantization=\"exl2\",\n",
        "          enforce_eager=True)\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)"
      ],
      "metadata": {
        "id": "AFwRQb1HwFqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! That should've generated some text to our prompts. Now, let's view the prompts."
      ],
      "metadata": {
        "id": "r2kY1k9awrxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for output in outputs:\n",
        "    print(f\"Prompt: {output.prompt}\")\n",
        "    print(f\"Generated text: {output.outputs[0].text}\\n\")"
      ],
      "metadata": {
        "id": "_fyLd0Mwwv7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To kill the engine and free up memory, run this."
      ],
      "metadata": {
        "id": "8mxF48g35Tb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch; import gc; from aphrodite.modeling.megatron.parallel_state import destroy_model_parallel\n",
        "\n",
        "destroy_model_parallel()\n",
        "del llm.llm_engine.driver_worker\n",
        "del llm\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.distributed.destroy_process_group()"
      ],
      "metadata": {
        "id": "7v1LAEAC4h56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}