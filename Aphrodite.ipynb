{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "name": "Aphrodite.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlpinDale/misc-scripts/blob/main/Aphrodite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Aphrodite Engine Colab!\n",
        "\n",
        "If you're on mobile, please tab on the play button below. If you're not, you can safely skip it and go to the next cell.\n",
        "\n",
        "The default model is Mythalion-13B, but you can either type in your model name, or select one of the defaults in the dropdown.\n",
        "\n",
        "If you're running a non-quantized model, please set the Quantization to `None` instead. AWQ quantization is also available, but you will need Colab Pro with better GPUs.\n"
      ],
      "metadata": {
        "id": "bN4MWt2g6E9X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewkXkyiFP2Hq"
      },
      "source": [
        "#@title <b>v-- Tap this if you play on Mobile</b> { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (You can ignore this step if you used run all and are on PC)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <b>v-- Currently only GPTQ quantization is supported. For AWQ, please upgrade to Colab Pro</b>\n",
        "#@markdown Currently only GPTQ quantization is supported. For AWQ, please upgrade to Colab Pro.\n",
        "#@markdown You can enter a custom model as well (for models bigger than 13B GPTQ, you will need Colab Pro).\n",
        "\n",
        "Model = \"TheBloke/Mythalion-13B-GPTQ\" #@param [\"TheBloke/Mythalion-13B-GPTQ\", \"TheBloke/MythoMax-L2-13B-GPTQ\", \"TheBloke/Pygmalion-2-13B\", \"TheBloke/MLewd-L2-Chat-13B-GPTQ\"]{allow-input: true}\n",
        "Quantization = \"gptq\" #@param ['gptq', \"None\"]\n",
        "!pip uninstall aphrodite-engine -y\n",
        "!wget -O aphrodite_engine-0.3.4-cp310-cp310-manylinux1_x86_64.whl http://0x0.st/HWnW.whl && pip install aphrodite_engine-0.3.4-cp310-cp310-manylinux1_x86_64.whl\n",
        "!wget -c https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://127.0.0.1:2242 &\n",
        "!sleep 10\n",
        "!cat nohup.out\n",
        "!python -m aphrodite.endpoints.api_server_kobold --model $Model --dtype float16 --host 127.0.0.1 --quantization gptq --gpu-memory-utilization 0.88 &\n"
      ]
    }
  ]
}
